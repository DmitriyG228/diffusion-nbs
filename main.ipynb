{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a90cf5-5d09-4562-b4e6-b0d400b952cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566fc74-75bb-414f-8282-37919042385b",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702fc68-1dc7-48b3-bf9f-a5094572b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers import logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "from tqdm.auto import tqdm\n",
    "from torch import autocast\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "from torchvision import transforms as tfms\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import pandas as pd\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import CLIPVisionModel,CLIPProcessor\n",
    "\n",
    "from mytools import tools as mt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42e6a9-d5d0-4b70-81c7-d363a661a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def select_cuda():\n",
    "    import  py3nvml.py3nvml as nvidia_smi\n",
    "\n",
    "    nvidia_smi.nvmlInit()\n",
    "\n",
    "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "    dfs =[]\n",
    "    for i in range(deviceCount):\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        dfs.append( pd.DataFrame([i, 100*info.free/info.total],\n",
    "                 index = ['device', 'free']).T)\n",
    "    devices = pd.concat(dfs).astype('int64').set_index('device')\n",
    "    nvidia_smi.nvmlShutdown()\n",
    "\n",
    "    return devices[devices['free']==devices['free'].max()].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77471c2-fcd1-4345-97b7-aee49ec1fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Set device\n",
    "torch_device = select_cuda()\n",
    "\n",
    "# Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "# The noise scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "# To the GPU we go!\n",
    "vae = vae.to(torch_device)\n",
    "text_encoder = text_encoder.to(torch_device)\n",
    "unet = unet.to(torch_device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34712e4-0ad5-47e8-9431-3e9f835040dc",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e555b9-9e67-47a3-bd71-4c1266fa88d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "batch_size = 1\n",
    "\n",
    "height = 32                        # default height of Stable Diffusion\n",
    "width = 32                         # default width of Stable Diffusion\n",
    "num_inference_steps = 53            # Number of denoising steps\n",
    "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "generator = torch.manual_seed(3)    # Seed generator to create the inital latent noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc457d0-6541-4f45-b9ac-b685a5604c5a",
   "metadata": {},
   "source": [
    "###\n",
    "1 text to emb\n",
    "2 img to latents\n",
    "3 emb and lats to img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11017b8-63cf-4ee4-b5bf-6a77e9cb71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def text2vector(prompt,negative_prompt = \"\"):\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = tokenizer(\n",
    "        [negative_prompt] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0] \n",
    "    return torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf2696-15f3-40a8-b491-538fe0b3ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_latents(seed=3):\n",
    "    generator = torch.manual_seed(seed)\n",
    "    # Prep Scheduler\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    # Prep latents\n",
    "    latents = torch.randn(\n",
    "      (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "      generator=generator,\n",
    "    )\n",
    "    latents = latents.to(torch_device)\n",
    "    return latents * scheduler.init_noise_sigma # Scaling (previous versions did latents = latents * self.scheduler.sigmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794b1aa-d3f2-44d5-9a19-8e4d96333765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
    "    return 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cfc346-11bf-41c3-aacb-e4089894985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def latents_to_pil(latents):\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    # Display\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    return [Image.fromarray(image) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42bd14-34ff-45c1-a482-d20fdcd77911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def img2latents(img,start_step):\n",
    "    encoded = pil_to_latent(img)\n",
    "    start_sigma = scheduler.sigmas[start_step]\n",
    "    noise = torch.randn_like(encoded)\n",
    "    latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
    "    return latents.to(torch_device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6de47d-712b-4aa1-b91e-266eb85d9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def loop(text_embeddings,latents,start_step=0,guidance_scale=7.5,nograd = True):\n",
    "    start_step =start_step-1\n",
    "    with autocast(\"cuda\"):\n",
    "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            if i > start_step: # << This is the only modification to the loop we do\n",
    "                # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                latent_model_input = torch.cat([latents] * 2)\n",
    "                sigma = scheduler.sigmas[i]\n",
    "                # Scale the latents (preconditioning):\n",
    "                # latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5) # Diffusers 0.3 and below\n",
    "                latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                \n",
    "                \n",
    "                \n",
    "                if nograd:\n",
    "                    with torch.no_grad():\n",
    "                        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                        \n",
    "                else:\n",
    "                    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                    \n",
    "\n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample       \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e0121-0ff4-4b6d-a7b4-a264702345e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e48d9-02ef-450d-8b33-e0d21ab0d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13218103-162d-4188-9134-281c5c85786e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAIRElEQVR4nHWVya9dRxHGa+o+59zh3Xff/J5jYjsDQxhiFtkgQUCMEiwhEsMWJNb8O0hBbBALFggBAkUsEhIgSBAntoKDwHIIHt58h3fO6e6qYnHtJb3p3nRVqepX34c/ffnH043N9enmcDiKVcXMiOju4KZFj48enpychFi7lRAEEBEQEMDBwc2s5FxKLqXknPqunc/ODh/eH43Xn372ucnapOReXn/l11XTDEbjyfrmdHNrY3N7fWNrbW0yGA4lVHuXLo/WJufn56o6nU5FxAERAQBXl7ubmbubqWnpu/bOv24vlsvRaCwMaCiW2lCFkw/u3Hn37WJYDYbjtfXpdHNza3s0Hu9fvvLRj3+qqZu2a0OMzAIA5oC4SuAA4A6rByKOxpPJdPPs5LipmyqGWNeyd3AphgpYdnb2T04O33///X/f/sdssZxs7nz1q19Ggo9+4nqIUUJIOd351z/a5WI8rA2gL4ZECISIiOQOajqdbu7u7h9cegKRWOTNP70u48n0g//e/90rrw0Hgy9+/jMvXL7SLpe3bt18eHqxvrHfDEar6ojYzT/451/awzt1VYGEZW9IYRil7XoFXCy7nPPV5144OHgCAAAQAQ7v35PBaPzMs+Otne27dz9o+7S1vSWx/tCTT1J1yEKhasyMEF2TMBNBV3LSMhivj6u6L1pK65qLaoiBzSYDRiSwAggAHGOU0XgyWptu7aar1566WMwBPBc9PToaDJbILBIc4OTB3eO3f7F7/Vt72ztbjTEJAoHEEGs1IAAt2dQu2uXacJhSOn7nl9Ody6Orn2FhkVgp+Nn5+d/fevtzn/vs0YN7w8GwHgxiVTEJMSMgoKX2GKzf2DygSR2rupQyny12drZT8aPj093tTWEufatx5JaXx7eG021CYiIqOc/ns/l8Bqbz2fmfX38tNoO6GcRYIREAAvhofW/vhe9X450+p6Kecln0eNLKgwUftXFWmvtzujez/56XWWtVVU2f+2a997x7IWYJIaDq+tr4+vWPI9jzn/40IdRVHWJE4hXsxEEGU5ZQsJ4v50jWW+VxcrYobqpOyRgdOo2BagCM4wMODQAQk9TNgFhyHsWqQoSdvYPc9xIkhECESISIy8Xi/bt31tfXh+uXmrV9IkJcrcJq3xCI8DE68/ns8PDhzu5+3TTCLID47rs3F+enV5965umPfDKl3mzGIRAzECEiEb1788att/5GzAhu7lVVCQus0gA6AEswMzfPOYUg4HbpQ1de/NLXmFlU9eDylXD1KWI+PTkCRHNnESJGJCQmlrXJ1M2quraS1dVKAZYYQkk9gJc+mYiqIqC7FbeSEwIRMxJJMxyP1ivVAuCqSkTMzCyPG0CIzMzCXHIB8Fg1uMIDkVjcAcliVSOimRGzmYE7C7s7EgkRpb5DsBtv3fj9b39z5dq1l779HUc0cEAAQEBwd0AgYQKoo+TUM5KbgRmxDIZDEXZwBzQzhEfSBA6IJMyyWF5UUfb2Dw4uPbG9s4skZlaKAqA/0gmKMTLhbLb8w6tvFDUSAQBmAXdmdjNHMPdPPPexZ65d6bqWRRwcEaVrl3fvvLe9vT0YNC9993vm2nUXqlpKAXhUjLlrv6xH4y6VV157A/7/2VifPnP1yqOPDogoi/np+mS8nM+Yl/Ozw7a9MNWHDx6cn54CgjuAg5kjBS0FEYXZ3FdQmhkiugMhAoC6iQR4HN7dAVG2tvdz7tuLBROZqtok9en87DyIuPlK7ZmIJQBxLmUlk+oAbpdHzbwYqZ0VXbmDmRGRcGARd0dAyqnLqScANzU3WJFDBAhmtkLCwVULICISgCPAx9bkC3vNT370xR8+/8TLP/j6i/trAgAAqjmnVHQ1PwBACVHcS1YzUwmNWem63h0e26Gv0hBLrBoALKoAMAS4PuLf//KN+WkH/765S72ZrfrOTEgEj61OctaScntxoaWrR3E1WzdzB1uFd8sp5VxyLpO1ta98/sWixZFvaGlTwT16uMSzazuffYrMbG9np+gjUh3AAQQAUt8BcjPezjnnnHJKq9pXRm6lABJQyDmNBs03vvKFWFUsoU9JiM3UkUQ4tV1KqRj0fQL3lYkioFjJpWRVdVVwRXBmMs059YT4CAZ3M2NiEc65AHFpOy2FRViilb7TQhLUoeRcVM3U3RFRtYhrIcTi1vcXiMSEFERVz8/Oz06PJMhyOe/7TksmiFWskqojk1DKBYEQsKgRsQFm9eXFRSkphNh3XUr94uxUck4OEGNd103qU58SET54ePzP2+9Rajd292/+7c2csqYutzMkyaWEIObgAIQIjuZubuieckJEMw0Sbvz1jdu33tGLc0GWwag+enD/r3989f5//kNEgDA7Of7wzqiytpzcO5wd5axa+lFTmYOaqbk7jpqYSmliyKpmRoREFJkEEJgTxw6wEpLjo+P3bt64ffOdrm1DCMIcGTaasOgshDioopAgc0lFyNWcIy/argphELAibCL0GYsCoRdNw6pi4TqGumnUYbm8kF/9/GelbxGxCoEQxLVmmfe9AzZBBGAggq5G0dGLu4ioVcyciq41Ed3ArY7i7k2kLmVRcxSJsL0xGjUsuWuFEdwiURM5Cqp5LcSIoMWZc8lCmNTWBlJVkrKWpABw0WerUIuZ2aip2y4NYjBzEQT3iz53fa4CCYJpsXEdhjWbw7IvjVATuMtazLEYoyLgeBDqipipa9tBoLbPYMW1rNY1MEJkIhzV0REdDUDny9ZrJEEfBBbCPmsuFgkEYdHlVFYiUQghsA9rcnc3FeGmYkKvAoNpIBSComVQS9HSp/7e0SmBg6WcUteV/wHndCU8Y2lvpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_img = Image.open('Y.png').resize((height,width));Y_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaefeaf-8244-4588-a608-512cd3d03517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_lat = pil_to_latent(Y_img)\n",
    "latents_to_pil(Y_lat)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d245e-4bbe-44ac-ae92-f23895993bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = text2vector('fireplace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6259c2-9921-4426-8b65-13d52a41d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = random_latents(234234)\n",
    "Y_hat.requires_grad= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b25d2-cc89-44e0-a19e-85ae6fb4f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(y_pred, y):\n",
    "    return torch.mean((y_pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745abe1c-28c3-4fd9-9406-dc93ed22024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e5de9-6862-4850-8341-f170e0a110a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    unet.zero_grad()\n",
    "    Y_hat_lat = loop(text_embeddings,Y_hat,nograd = False)\n",
    "    clear_output()\n",
    "    display(latents_to_pil(Y_hat_lat)[0])\n",
    "    loss = criterion(Y_hat_lat,Y_lat)\n",
    "    display(loss)\n",
    "    [g]  = torch.autograd.grad(loss, [Y_hat])\n",
    "    Y_hat.data -= learning_rate * len(Y_hat) * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85538528-041d-42aa-a485-d6e81a2bc45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cea530-94d3-4385-adb1-56efac534e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91164610-e4c1-4608-9801-e099e7302ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c129c-f7bd-4477-a584-bcf2ac1a5b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff",
   "language": "python",
   "name": "diff"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
